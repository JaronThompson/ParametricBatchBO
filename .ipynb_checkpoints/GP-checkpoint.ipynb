{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from armored.models import *\n",
    "from armored.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# used in GP objective\n",
    "from jax.scipy.stats.norm import cdf, pdf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "params = {'legend.fontsize': 18,\n",
    "          'figure.figsize': (8, 6),\n",
    "         'axes.labelsize': 24,\n",
    "         'axes.titlesize':24,\n",
    "         'axes.linewidth':5,\n",
    "         'xtick.labelsize':20,\n",
    "         'ytick.labelsize':20}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Gaussian process kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(xn, xm, variance):\n",
    "    # eqn 6.23 in PRML\n",
    "    return jnp.exp(- jnp.exp(variance[0]) * jnp.dot(xn-xm, xn-xm))\n",
    "\n",
    "# rbf kernel with vector for scaling \n",
    "# def kernel(xn, xm, inv_variance):\n",
    "#     # precision weighted norm \n",
    "#     return jnp.exp(-jnp.einsum('i,ij,j->', xn-xm, jnp.diag(jnp.exp(inv_variance)), xn-xm))\n",
    "\n",
    "# linear kernel \n",
    "# def kernel(xn, xm, weights):\n",
    "#     # precision weighted norm \n",
    "#     return jnp.einsum('i,ij,j->', xn, jnp.diag(weights), xm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define simulation parameters and import full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trials \n",
    "n_trials = 30\n",
    "\n",
    "# number of dtl cycles \n",
    "n_dtl  = 25\n",
    "\n",
    "# define number of initial samples to train on\n",
    "n_init = 5\n",
    "\n",
    "# number of samples for next experiment \n",
    "n_test = 1\n",
    "\n",
    "# number of species in model\n",
    "n_s = 5\n",
    "\n",
    "# number of resources\n",
    "n_r = 7\n",
    "\n",
    "# define all system variables \n",
    "inputs  = ['r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'x1', 'x2']\n",
    "outputs = ['p']\n",
    "\n",
    "# import data \n",
    "main_df = pd.read_csv(\"Data/reactor_ubiome_gp.csv\")\n",
    "exp_names = np.sort(main_df.Experiments.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine random sets of initial experiments\n",
    "initial_exps = [rng.choice(exp_names, n_init, replace=False) for _ in range(n_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute sum of squares error \n",
    "def sse(a, b):\n",
    "    return np.sum((a-b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init dataframe that stores DTL information\n",
    "dtl_df = pd.DataFrame()\n",
    "dtl_df_R = pd.DataFrame()\n",
    "dtl_df_sse = pd.DataFrame()\n",
    "elapsed_time = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    \n",
    "    # keep track of objective \n",
    "    objective_found = []\n",
    "    \n",
    "    # design matrix over entire design space\n",
    "    X_design  = np.array(main_df[inputs].values, float)\n",
    "    Y_design  = np.array(main_df[outputs].values, float)\n",
    "    exp_names = np.copy(main_df.Experiments.values)\n",
    "\n",
    "    # choose random set of training samples from design space\n",
    "    train_inds = np.in1d(exp_names, initial_exps[trial])\n",
    "    X_train = X_design[train_inds]\n",
    "    Y_train = Y_design[train_inds]\n",
    "\n",
    "    # remove training samples from design set\n",
    "    X_design  = X_design[~train_inds]\n",
    "    Y_design  = Y_design[~train_inds]\n",
    "\n",
    "    # compute objectives\n",
    "    objective_found.append(np.max(Y_train))\n",
    "    objective_rval = []\n",
    "    objective_sse  = []\n",
    "\n",
    "    # Search over full factorial and update model\n",
    "    for dtl in range(n_dtl):\n",
    "        print(f\"Running trial {trial+1}, cycle {dtl+1}\")\n",
    "\n",
    "        # scale X data\n",
    "        Xscaler = StandardScaler().fit(X_train)\n",
    "        X_train = Xscaler.transform(X_train)\n",
    "        X_test  = Xscaler.transform(X_design)\n",
    "        \n",
    "        # scale Y data\n",
    "        Yscaler = StandardScaler().fit(np.vstack(Y_train))\n",
    "        Y_train = Yscaler.transform(np.vstack(Y_train))\n",
    "\n",
    "        # init Gaussian process\n",
    "        # params = -1.*np.ones(X_train.shape[-1])\n",
    "        params = np.array([-1.])\n",
    "        gp = GP(kernel, params, beta=1.)\n",
    "\n",
    "        # fit to training data \n",
    "        gp.fit(X_train, Y_train)\n",
    "        \n",
    "        # assess fit\n",
    "        '''pred, _ = gp.predict(X_train)\n",
    "        rvalue = linregress(Y_train.ravel(), pred.ravel()).rvalue\n",
    "        sse_value = sse(Y_train.ravel(), pred.ravel())\n",
    "        plt.scatter(Y_train.ravel(), pred.ravel(), label=\"R = {:.3f}\\nSSE = {:.3f}\".format(rvalue, sse_value))\n",
    "        plt.legend()\n",
    "        plt.show()'''\n",
    "        \n",
    "        # assess prediction performance of end-point product\n",
    "        pred, _ = gp.predict(X_test)\n",
    "        pred = Yscaler.inverse_transform(np.vstack(pred)).ravel()\n",
    "        rvalue = linregress(Y_design.ravel(), pred).rvalue\n",
    "        sse_value = sse(Y_design.ravel(), pred)\n",
    "        '''plt.scatter(Y_design.ravel(), pred, label=\"R = {:.3f}\\nSSE = {:.3f}\".format(rvalue, sse_value))\n",
    "        plt.legend()\n",
    "        plt.show()'''\n",
    "        objective_rval.append(rvalue)\n",
    "        objective_sse.append(sse_value)\n",
    "        \n",
    "        # search over design space\n",
    "        \n",
    "        # Expected Improvement objective\n",
    "        fstar = np.max(Y_train)\n",
    "        def objective(pred, stdv):\n",
    "            improvement = pred - fstar\n",
    "            Z = improvement/stdv\n",
    "            return improvement*cdf(Z) + stdv*pdf(Z)\n",
    "\n",
    "        # search for n_test new points\n",
    "        new_experiment_inds = gp.search(X_test, objective, n_test)        \n",
    "\n",
    "        # collect new data \n",
    "        X_new = X_design[new_experiment_inds]\n",
    "        Y_new = Y_design[new_experiment_inds]\n",
    "\n",
    "        # remove training samples from main dataset\n",
    "        X_design = np.delete(X_design, new_experiment_inds, axis=0)\n",
    "        Y_design = np.delete(Y_design, new_experiment_inds, axis=0)\n",
    "\n",
    "        # store the best objective found (so far)\n",
    "        objective_found.append(np.max([np.max(objective_found), np.max(Y_new)]))\n",
    "        print(objective_found)\n",
    "\n",
    "        # Update dataset (unscaled)\n",
    "        X_train = np.concatenate((Xscaler.inverse_transform(X_train), X_new))\n",
    "        Y_train = np.concatenate((Yscaler.inverse_transform(Y_train), Y_new))\n",
    "        \n",
    "    ### fit model one last time to assess final prediction performance ### \n",
    "    # scale data\n",
    "    Xscaler = StandardScaler().fit(X_train)\n",
    "    X_train = Xscaler.transform(X_train)\n",
    "    X_test  = Xscaler.transform(X_design)\n",
    "\n",
    "    Yscaler = StandardScaler().fit(np.vstack(Y_train))\n",
    "    Y_train = Yscaler.transform(np.vstack(Y_train))\n",
    "\n",
    "    # init Gaussian process\n",
    "    gp = GP(kernel, params, beta=1.)\n",
    "\n",
    "    # fit to training data \n",
    "    gp.fit(X_train, Y_train)\n",
    "\n",
    "    # assess prediction performance of end-point product\n",
    "    pred, _ = gp.predict(X_test)\n",
    "    pred = Yscaler.inverse_transform(np.vstack(pred)).ravel()\n",
    "    rvalue = linregress(Y_design.ravel(), pred).rvalue\n",
    "    sse_value = sse(Y_design.ravel(), pred)\n",
    "    plt.scatter(Y_design.ravel(), pred, label=\"R = {:.3f}\\nSSE = {:.3f}\".format(rvalue, sse_value))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    objective_rval.append(rvalue)\n",
    "    objective_sse.append(sse_value)\n",
    "        \n",
    "    # save data to dataframe\n",
    "    dtl_df_i = pd.DataFrame()\n",
    "    dtl_df_i['Trial'] = [trial]\n",
    "    for j,obj_found in enumerate(objective_found):\n",
    "        dtl_df_i[f'DTL {j}'] = [obj_found]\n",
    "    dtl_df = pd.concat((dtl_df, dtl_df_i))\n",
    "    \n",
    "    # save data to dataframe\n",
    "    dtl_df_r = pd.DataFrame()\n",
    "    dtl_df_r['Trial'] = [trial]\n",
    "    for j,r_val in enumerate(objective_rval):\n",
    "        dtl_df_r[f'DTL {j}'] = [r_val]\n",
    "    dtl_df_R = pd.concat((dtl_df_R, dtl_df_r))\n",
    "    \n",
    "    # save data to dataframe\n",
    "    dtl_df_e = pd.DataFrame()\n",
    "    dtl_df_e['Trial'] = [trial]\n",
    "    for j,e in enumerate(objective_sse):\n",
    "        dtl_df_e[f'DTL {j}'] = [e]\n",
    "    dtl_df_sse = pd.concat((dtl_df_sse, dtl_df_e))\n",
    "    \n",
    "    dtl_df.to_csv(\"results/GP.csv\", index=False)\n",
    "    dtl_df_R.to_csv(\"results/GP_rvals.csv\", index=False)\n",
    "    dtl_df_sse.to_csv(\"results/GP_sse.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl_df_R.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl_df_sse.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
